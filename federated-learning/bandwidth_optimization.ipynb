{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0864134",
   "metadata": {},
   "source": [
    "# Bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f578c4",
   "metadata": {},
   "source": [
    "Add in `requirements.txt` file:\n",
    "```\n",
    "flwr==1.10.0\n",
    "ray==2.6.3\n",
    "flwr-datasets[vision]==0.2.0\n",
    "torch==2.2.1\n",
    "torchvision==0.17.1\n",
    "matplotlib==3.8.3\n",
    "scikit-learn==1.4.2\n",
    "seaborn==0.13.2\n",
    "ipywidgets==8.1.2\n",
    "transformers==4.42.4\n",
    "accelerate==0.30.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b96f7b",
   "metadata": {},
   "source": [
    "#### 1. Load imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bc861ea-21bb-45a8-8196-a5b0d0ace374",
   "metadata": {
    "height": 1679
   },
   "outputs": [],
   "source": [
    "from flwr.client.mod import parameters_size_mod\n",
    "\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "from logging import INFO\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common.logger import (\n",
    "    ConsoleHandler,\n",
    "    console_handler,\n",
    "    FLOWER_LOGGER,\n",
    "    LOG_COLORS,\n",
    ")\n",
    "from logging import LogRecord\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from flwr.server import ServerAppComponents\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.client.mod import parameters_size_mod\n",
    "from flwr.common import (\n",
    "    Context,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    MessageType,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    Context,\n",
    "    parameters_to_ndarrays,\n",
    "    ndarrays_to_parameters,\n",
    ")\n",
    "from flwr.common.logger import (\n",
    "    console_handler,\n",
    "    log,\n",
    "    update_console_handler,\n",
    ")\n",
    "from flwr.server import ClientManager, ServerApp, ServerConfig\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr.simulation import run_simulation\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, GPTNeoXForCausalLM\n",
    "\n",
    "\n",
    "# Customize logging for the course.\n",
    "class InfoFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return record.levelno == INFO\n",
    "\n",
    "\n",
    "FLOWER_LOGGER.removeHandler(console_handler)\n",
    "\n",
    "# To filter logging coming from the Simulation Engine\n",
    "# so it's more readable in notebooks\n",
    "from logging import ERROR\n",
    "backend_setup = {\"init_args\": {\"logging_level\": ERROR, \"log_to_driver\": True}}\n",
    "\n",
    "class ConsoleHandlerV2(ConsoleHandler):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def format(self, record: LogRecord) -> str:\n",
    "        \"\"\"Format function that adds colors to log level.\"\"\"\n",
    "        if self.json:\n",
    "            log_fmt = \"{lvl='%(levelname)s', time='%(asctime)s', msg='%(message)s'}\"\n",
    "        else:\n",
    "            log_fmt = (\n",
    "                f\"{LOG_COLORS[record.levelname] if self.colored else ''}\"\n",
    "                f\"%(levelname)s {'%(asctime)s' if self.timestamps else ''}\"\n",
    "                f\"{LOG_COLORS['RESET'] if self.colored else ''}\"\n",
    "                f\": %(message)s\"\n",
    "            )\n",
    "        formatter = logging.Formatter(log_fmt)\n",
    "        return formatter.format(record)\n",
    "\n",
    "\n",
    "console_handlerv2 = ConsoleHandlerV2(\n",
    "    timestamps=False,\n",
    "    json=False,\n",
    "    colored=True,\n",
    ")\n",
    "console_handlerv2.setLevel(INFO)\n",
    "console_handlerv2.addFilter(InfoFilter())\n",
    "FLOWER_LOGGER.addHandler(console_handlerv2)\n",
    "\n",
    "\n",
    "def set_weights(net, parameters):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict(\n",
    "        {k: torch.tensor(v) for k, v in params_dict}\n",
    "    )\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def get_weights(net):\n",
    "    ndarrays = [\n",
    "        val.cpu().numpy() for _, val in net.state_dict().items()\n",
    "    ]\n",
    "    return ndarrays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a45205f",
   "metadata": {},
   "source": [
    "#### 2.  Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d9e47",
   "metadata": {},
   "source": [
    "*  Initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb82dbc0-cb85-408c-ac0f-265af010bfea",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/pythia-14m\",\n",
    "    cache_dir=\"./pythia-14m/cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b51a54",
   "metadata": {},
   "source": [
    "Find more information about [EleutherAI/pythia-14m](https://huggingface.co/EleutherAI/pythia-14m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6159b23d",
   "metadata": {},
   "source": [
    "* Get some Model values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "973616a4-8b73-41b8-bc9d-4abbbc74502d",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m: Model size is: 53 MB\n",
      "\u001b[92mINFO \u001b[0m: Model size is: 53 MB\n"
     ]
    }
   ],
   "source": [
    "vals = model.state_dict().values()\n",
    "total_size_bytes = sum(p.element_size() * p.numel() for p in vals)\n",
    "total_size_mb = int(total_size_bytes / (1024**2))\n",
    "\n",
    "log(INFO, \"Model size is: {} MB\".format(total_size_mb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccf308f",
   "metadata": {},
   "source": [
    "* Define the FlowerClient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea907386-ef03-46a5-bbe3-05414dee7b4b",
   "metadata": {
    "height": 404
   },
   "outputs": [],
   "source": [
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, net):\n",
    "        self.net = net\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        set_weights(self.net, parameters)\n",
    "        # No actual training here\n",
    "        return get_weights(self.net), 1, {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        set_weights(self.net, parameters)\n",
    "        # No actual evaluation here\n",
    "        return float(0), int(1), {\"accuracy\": 0}\n",
    "\n",
    "\n",
    "def client_fn(context: Context) -> FlowerClient:\n",
    "    return FlowerClient(model).to_client()\n",
    "\n",
    "\n",
    "client = ClientApp(\n",
    "    client_fn,\n",
    "    mods=[parameters_size_mod],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffa0f64",
   "metadata": {},
   "source": [
    "* Define the custom strategy: BandwidthTrackingFedAvg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95213409-e47d-4415-b69f-9bfb5498aaf4",
   "metadata": {
    "height": 557
   },
   "outputs": [],
   "source": [
    "bandwidth_sizes = []\n",
    "\n",
    "\n",
    "class BandwidthTrackingFedAvg(FedAvg):\n",
    "    def aggregate_fit(self, server_round, results, failures):\n",
    "        if not results:\n",
    "            return None, {}\n",
    "\n",
    "        # Track sizes of models received\n",
    "        for _, res in results:\n",
    "            ndas = parameters_to_ndarrays(res.parameters)\n",
    "            size = int(sum(n.nbytes for n in ndas) / (1024**2))\n",
    "            log(INFO, f\"Server receiving model size: {size} MB\")\n",
    "            bandwidth_sizes.append(size)\n",
    "\n",
    "        # Call FedAvg for actual aggregation\n",
    "        return super().aggregate_fit(server_round, results, failures)\n",
    "\n",
    "    def configure_fit(self, server_round, parameters, client_manager):\n",
    "        # Call FedAvg for actual configuration\n",
    "        instructions = super().configure_fit(\n",
    "            server_round, parameters, client_manager\n",
    "        )\n",
    "\n",
    "        # Track sizes of models to be sent\n",
    "        for _, ins in instructions:\n",
    "            ndas = parameters_to_ndarrays(ins.parameters)\n",
    "            size = int(sum(n.nbytes for n in ndas) / (1024**2))\n",
    "            log(INFO, f\"Server sending model size: {size} MB\")\n",
    "            bandwidth_sizes.append(size)\n",
    "\n",
    "        return instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0166979-2cc2-44ac-89dd-e9bc05c941e9",
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "params = ndarrays_to_parameters(get_weights(model))\n",
    "\n",
    "def server_fn(context: Context):\n",
    "    strategy = BandwidthTrackingFedAvg(\n",
    "        fraction_evaluate=0.0,\n",
    "        initial_parameters=params,\n",
    "    )\n",
    "    config = ServerConfig(num_rounds=1)\n",
    "    return ServerAppComponents(\n",
    "        strategy=strategy,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "\n",
    "server = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4e469b",
   "metadata": {},
   "source": [
    "* Run the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4cddbee-f017-4ad1-b6e1-2a81b857fcef",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m: Starting Flower ServerApp, config: num_rounds=1, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m: Starting Flower ServerApp, config: num_rounds=1, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m: \n",
      "\u001b[92mINFO \u001b[0m: \n",
      "\u001b[92mINFO \u001b[0m: [INIT]\n",
      "\u001b[92mINFO \u001b[0m: [INIT]\n",
      "\u001b[92mINFO \u001b[0m: Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m: Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m: Evaluating initial global parameters\n",
      "\u001b[92mINFO \u001b[0m: Evaluating initial global parameters\n",
      "\u001b[92mINFO \u001b[0m: \n",
      "\u001b[92mINFO \u001b[0m: \n",
      "\u001b[92mINFO \u001b[0m: [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m: [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m: Server sending model size: 53 MB\n",
      "\u001b[92mINFO \u001b[0m: Server sending model size: 53 MB\n",
      "\u001b[92mINFO \u001b[0m: Server sending model size: 53 MB\n",
      "\u001b[92mINFO \u001b[0m: Server sending model size: 53 MB\n",
      "\u001b[92mINFO \u001b[0m: configure_fit: strategy sampled 2 clients (out of 2)\n",
      "\u001b[92mINFO \u001b[0m: configure_fit: strategy sampled 2 clients (out of 2)\n",
      "\u001b[2m\u001b[36m(ClientAppActor pid=845)\u001b[0m \u001b[92mINFO \u001b[0m:      {'fitins.parameters': {'parameters': 14067712, 'bytes': 56280718}}\n",
      "\u001b[2m\u001b[36m(ClientAppActor pid=845)\u001b[0m \u001b[92mINFO \u001b[0m:      Total parameters transmitted: 56280718 bytes\n",
      "2025-01-12 16:58:30,934\tWARNING worker.py:2037 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff2666a2f28b1f04865bf92e4701000000 Worker ID: b3b12dc5fd037891c3a8ef79e23641a71cf951b5faec6f34cb55933a Node ID: 2936045e303a5a23123366e575bc1508edac4e5890df0fd6e39eecbd Worker IP address: 172.29.0.87 Worker port: 45713 Worker PID: 845 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[92mINFO \u001b[0m: aggregate_fit: received 0 results and 2 failures\n",
      "\u001b[92mINFO \u001b[0m: aggregate_fit: received 0 results and 2 failures\n",
      "\u001b[92mINFO \u001b[0m: configure_evaluate: no clients selected, skipping evaluation\n",
      "\u001b[92mINFO \u001b[0m: configure_evaluate: no clients selected, skipping evaluation\n",
      "\u001b[92mINFO \u001b[0m: \n",
      "\u001b[92mINFO \u001b[0m: \n",
      "\u001b[92mINFO \u001b[0m: [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m: [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m: Run finished 1 round(s) in 9.33s\n",
      "\u001b[92mINFO \u001b[0m: Run finished 1 round(s) in 9.33s\n",
      "\u001b[92mINFO \u001b[0m: \n",
      "\u001b[92mINFO \u001b[0m: \n",
      "\u001b[2m\u001b[36m(ClientAppActor pid=846)\u001b[0m \u001b[92mINFO \u001b[0m:      {'fitins.parameters': {'parameters': 14067712, 'bytes': 56280718}}\n",
      "\u001b[2m\u001b[36m(ClientAppActor pid=846)\u001b[0m \u001b[92mINFO \u001b[0m:      Total parameters transmitted: 56280718 bytes\n"
     ]
    }
   ],
   "source": [
    "run_simulation(server_app=server,\n",
    "               client_app=client,\n",
    "               num_supernodes=2,\n",
    "               backend_config=backend_setup\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d64b93",
   "metadata": {},
   "source": [
    "* Log how much bandwidth was used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65359bf",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d49b2f6d-91eb-4161-b86e-74d897cb725a",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m: Total bandwidth used: 106 MB\n",
      "\u001b[92mINFO \u001b[0m: Total bandwidth used: 106 MB\n"
     ]
    }
   ],
   "source": [
    "log(INFO, \"Total bandwidth used: {} MB\".format(sum(bandwidth_sizes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470289af",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3542f1",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

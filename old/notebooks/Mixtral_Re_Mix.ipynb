{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanikamal/LLM-Playground/blob/main/notebooks/Mixtral_Re_Mix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObiEP0LqXwTa"
      },
      "source": [
        "# Fine-tuning Mixtral with QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEPNqoIsCBKE"
      },
      "outputs": [],
      "source": [
        "!pip install -qU flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NBw3rJdCVhG"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate bitsandbytes peft -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkqrWQY3X1pi"
      },
      "source": [
        "> NOTE: We will be downloading the model's whopping ~5*19GBs of weights - and then loading those (in 4-bit quantization) into our GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1d364b48b66e4fc981231cf40d46ff25",
            "e40a3f3b4c4f4502bee5c0a7ff5a6637",
            "0c846d0d2a594109abab3c727f0d60f5",
            "3e1f08661dc7496c91bee7b36d5f50a6",
            "b00a2dacab4f45589bc413de44d79b16",
            "b42acbb30b7143b2839beb850cc2b839",
            "28f46226871944fca2cd463639a478d2",
            "1883a28ac6be43ffae19fd799c38cf4e",
            "1a9a289d36144abb80222e80b6454f48",
            "949a69c1cc4c4deea1f3ce68e7beb315",
            "ccc209627092455abbb0e2a33ebbb133"
          ]
        },
        "id": "cjnE3TgKCZrd",
        "outputId": "91822360-76b1-4d8a-aede-160da8fe25aa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d364b48b66e4fc981231cf40d46ff25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "bits_and_bytes_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "mixtral_7B = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bits_and_bytes_config,\n",
        "    attn_implementation=\"flash_attention_2\"\n",
        ")\n",
        "\n",
        "mixtral_tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8C2sv8pS9bW",
        "outputId": "55fd3efa-3ed2-4a0f-8d79-186af0961148"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1636: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Instruction:\n",
            "Use the provided input to create an instruction that could have been used to generate the response with an LLM.### Input:\n",
            "There are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\n",
            "\n",
            "### Response:\n",
            "Based on the information provided, you could create an instruction like this:\n",
            "\n",
            "\"Generate a response describing the characteristics of four common types of grass: Kentucky Bluegrass, Ryegrass, Fescues, and Bermuda grass. Mention that Kentucky Bluegrass is the most common due to its quick and easy growth and soft texture. Describe Ryegrass as shiny and bright green. Characterize Fescues as dark green and shiny. And explain that Bermuda grass is harder and can grow in drier soil.\"\n",
            "\n",
            "The LLM would then generate a response similar to the original, based on this instruction.\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "text = \"### Instruction:\\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.### Input:\\nThere are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\\n\\n### Response:\"\n",
        "inputs = mixtral_tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = mixtral_7B.generate(**inputs, max_new_tokens=150)\n",
        "print(mixtral_tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0jG6gQNXIDb",
        "outputId": "63f22863-89c1-4d27-8280-40aa2d26d91d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mixtral_tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFmWlA3TWScD"
      },
      "outputs": [],
      "source": [
        "!pip install -qU datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKdSgCyGWTk4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "instruct_tune_dataset = load_dataset(\"mosaicml/instruct-v3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttM8EtG0WaSo"
      },
      "outputs": [],
      "source": [
        "instruct_tune_dataset = instruct_tune_dataset.filter(lambda x: x[\"source\"] == \"dolly_hhrlhf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnbmScZOWbVk",
        "outputId": "c4f24180-0947-46df-d5cf-25475b8504d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['prompt', 'response', 'source'],\n",
              "        num_rows: 34333\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['prompt', 'response', 'source'],\n",
              "        num_rows: 4771\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "instruct_tune_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzZbqeUtWckO"
      },
      "outputs": [],
      "source": [
        "instruct_tune_dataset[\"train\"] = instruct_tune_dataset[\"train\"].select(range(5_000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_UJCtElWdkc"
      },
      "outputs": [],
      "source": [
        "instruct_tune_dataset[\"test\"] = instruct_tune_dataset[\"test\"].select(range(200))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDqfC0bwWeYc",
        "outputId": "9c3c1b7e-037a-4558-bdae-dcb2d87b4f28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['prompt', 'response', 'source'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['prompt', 'response', 'source'],\n",
              "        num_rows: 200\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "instruct_tune_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR5TRF9vWfbO"
      },
      "outputs": [],
      "source": [
        "def create_prompt(sample):\n",
        "  bos_token = \"<s>\"\n",
        "  original_system_message = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "  system_message = \"[INST]Use the provided input to create an instruction that could have been used to generate the response with an LLM.\"\n",
        "  response = sample[\"prompt\"].replace(original_system_message, \"\").replace(\"\\n\\n### Instruction\\n\", \"\").replace(\"\\n### Response\\n\", \"\").strip()\n",
        "  input = sample[\"response\"]\n",
        "  eos_token = \"</s>\"\n",
        "\n",
        "  full_prompt = \"\"\n",
        "  full_prompt += bos_token\n",
        "  full_prompt += system_message\n",
        "  full_prompt += \"\\n\" + input\n",
        "  full_prompt += \"[/INST]\"\n",
        "  full_prompt += response\n",
        "  full_prompt += eos_token\n",
        "\n",
        "  return full_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TDYstfWoWjMh",
        "outputId": "096884b3-d76b-491d-c5b0-56f52ff5b30d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<s>[INST]Use the provided input to create an instruction that could have been used to generate the response with an LLM.\\nThere are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.[/INST]What are different types of grass?</s>'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "create_prompt(instruct_tune_dataset[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY5iGppcWnX2"
      },
      "outputs": [],
      "source": [
        "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUIJ4KJGWovu"
      },
      "outputs": [],
      "source": [
        "model = prepare_model_for_kbit_training(mixtral_7B)\n",
        "model = get_peft_model(mixtral_7B, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRQdX4KFW46t"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "  output_dir = \"mistral_instruct_generation\",\n",
        "  #num_train_epochs=5,\n",
        "  max_steps = 100, # comment out this line if you want to train in epochs\n",
        "  per_device_train_batch_size = 2,\n",
        "  warmup_steps = 0.03,\n",
        "  logging_steps=10,\n",
        "  save_strategy=\"epoch\",\n",
        "  #evaluation_strategy=\"epoch\",\n",
        "  evaluation_strategy=\"steps\",\n",
        "  eval_steps=20, # comment out this line if you want to evaluate at the end of each epoch\n",
        "  learning_rate=2e-4,\n",
        "  bf16=True,\n",
        "  lr_scheduler_type='constant',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jms2PD1W8Qk"
      },
      "outputs": [],
      "source": [
        "!pip install -qU trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JGS_mYWW6fG",
        "outputId": "5059b501-a6ba-43f6-b8c3-0ee84632b3e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:548: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictionary returned by each element of the dataset. Make sure you know what you are doing.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:247: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:267: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "max_seq_length = 2048\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "  model=model,\n",
        "  peft_config=peft_config,\n",
        "  max_seq_length=max_seq_length,\n",
        "  tokenizer=mixtral_tokenizer,\n",
        "  packing=True,\n",
        "  formatting_func=create_prompt,\n",
        "  args=args,\n",
        "  train_dataset=instruct_tune_dataset[\"train\"],\n",
        "  eval_dataset=instruct_tune_dataset[\"test\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "Xx4lwzIvXEEx",
        "outputId": "d18a9385-1293-490b-84af-e3ddcc94b95c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:570: UserWarning: The dataset reached end and the iterator is reset to the start.\n",
            "  warnings.warn(\"The dataset reached end and the iterator is reset to the start.\")\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 07:56, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.311400</td>\n",
              "      <td>4.257724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.494100</td>\n",
              "      <td>3.227834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>3.643000</td>\n",
              "      <td>3.174503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>3.252400</td>\n",
              "      <td>2.929174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.150600</td>\n",
              "      <td>2.941102</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checkpoint destination directory mistral_instruct_generation/checkpoint-100 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=100, training_loss=3.360270538330078, metrics={'train_runtime': 482.8119, 'train_samples_per_second': 0.414, 'train_steps_per_second': 0.207, 'total_flos': 1.145886637817856e+17, 'train_loss': 3.360270538330078, 'epoch': 0.04})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYITJzuebZR1",
        "outputId": "d418a42f-1988-490c-8d69-6bcb32f034c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py:229: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "merged_model = model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBJkNgC9b2hf",
        "outputId": "107c814c-5bb7-4e90-837e-4aebeb152089"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
            "There are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.[/INST]\n",
            "\n",
            "Ryegrass is a type of grass. It is a type of grass that is a type of grass that is a type of grass that is a type of grass that is a type of grass that is a type of grass that is a type of grass that is a type of grass that is a type of grass that is a type of grass that is a type of grass that is a type of grass that is a type of grass that is a type of grass that is a type of grass that is a type of grass that is a type of grass that is a type of grass that is a type of grass that is a type of grass that is a type of grass that is a type of grass that is a type of grass\n"
          ]
        }
      ],
      "source": [
        "text = \"<s>[INST]Use the provided input to create an instruction that could have been used to generate the response with an LLM.\\nThere are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.[/INST]\"\n",
        "inputs = mixtral_tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = merged_model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=150,\n",
        "    generation_kwargs={\"repetition_penalty\" : 1.7}\n",
        ")\n",
        "print(mixtral_tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c846d0d2a594109abab3c727f0d60f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1883a28ac6be43ffae19fd799c38cf4e",
            "max": 19,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a9a289d36144abb80222e80b6454f48",
            "value": 19
          }
        },
        "1883a28ac6be43ffae19fd799c38cf4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a9a289d36144abb80222e80b6454f48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d364b48b66e4fc981231cf40d46ff25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e40a3f3b4c4f4502bee5c0a7ff5a6637",
              "IPY_MODEL_0c846d0d2a594109abab3c727f0d60f5",
              "IPY_MODEL_3e1f08661dc7496c91bee7b36d5f50a6"
            ],
            "layout": "IPY_MODEL_b00a2dacab4f45589bc413de44d79b16"
          }
        },
        "28f46226871944fca2cd463639a478d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e1f08661dc7496c91bee7b36d5f50a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_949a69c1cc4c4deea1f3ce68e7beb315",
            "placeholder": "​",
            "style": "IPY_MODEL_ccc209627092455abbb0e2a33ebbb133",
            "value": " 19/19 [06:35&lt;00:00, 19.49s/it]"
          }
        },
        "949a69c1cc4c4deea1f3ce68e7beb315": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b00a2dacab4f45589bc413de44d79b16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b42acbb30b7143b2839beb850cc2b839": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccc209627092455abbb0e2a33ebbb133": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e40a3f3b4c4f4502bee5c0a7ff5a6637": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b42acbb30b7143b2839beb850cc2b839",
            "placeholder": "​",
            "style": "IPY_MODEL_28f46226871944fca2cd463639a478d2",
            "value": "Loading checkpoint shards: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac959c4b-d7a9-4949-9c39-2ab46fbf82cd",
   "metadata": {},
   "source": [
    "# Getting started with Llama 2 & 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df1e2b-2079-4dee-84e9-77e13dd31e05",
   "metadata": {},
   "source": [
    "The code to call the Llama 2 & 3 models through the Together.ai hosted API service has been wrapped into a helper function called `llama`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeabc67",
   "metadata": {
    "height": 30
   },
   "source": [
    "Add to requirements.txt file:\n",
    "```\n",
    "together==0.2.10\n",
    "python-dotenv==1.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10075542-6019-40b6-b3c1-532383e4a6dd",
   "metadata": {
    "height": 1152
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import warnings\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Initailize global variables\n",
    "_ = load_dotenv(find_dotenv())\n",
    "# warnings.filterwarnings('ignore')\n",
    "url = f\"{os.getenv('DLAI_TOGETHER_API_BASE', 'https://api.together.xyz')}/inference\"\n",
    "headers = {\n",
    "        \"Authorization\": f\"Bearer {os.getenv('TOGETHER_API_KEY')}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "\n",
    "import time\n",
    "def llama(prompt, \n",
    "          add_inst=True, \n",
    "          model=\"togethercomputer/llama-2-7b-chat\", \n",
    "          temperature=0.0, \n",
    "          max_tokens=1024,\n",
    "          verbose=False,\n",
    "          url=url,\n",
    "          headers=headers,\n",
    "          base = 2, # number of seconds to wait\n",
    "          max_tries=3):\n",
    "    \n",
    "    if add_inst:\n",
    "        prompt = f\"[INST]{prompt}[/INST]\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Prompt:\\n{prompt}\\n\")\n",
    "        print(f\"model: {model}\")\n",
    "\n",
    "    data = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "\n",
    "    # Allow multiple attempts to call the API incase of downtime.\n",
    "    # Return provided response to user after 3 failed attempts.    \n",
    "    wait_seconds = [base**i for i in range(max_tries)]\n",
    "\n",
    "    for num_tries in range(max_tries):\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "            return response.json()['output']['choices'][0]['text']\n",
    "        except Exception as e:\n",
    "            if response.status_code != 500:\n",
    "                return response.json()\n",
    "\n",
    "            print(f\"error message: {e}\")\n",
    "            print(f\"response object: {response}\")\n",
    "            print(f\"num_tries {num_tries}\")\n",
    "            print(f\"Waiting {wait_seconds[num_tries]} seconds before automatically trying again.\")\n",
    "            time.sleep(wait_seconds[num_tries])\n",
    "            \n",
    "    print(f\"Tried {max_tries} times to make API call to get a valid response object\")\n",
    "    print(\"Returning provided response\")\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a549fd-dd50-4ae1-a5d8-ce00fdc707a3",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# define the prompt\n",
    "prompt = \"Help me write a birthday card for my dear friend Andrew.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b89f4c48-363d-4ddb-8f20-215bbd47004a",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Unable to access non-serverless model togethercomputer/llama-2-7b-chat. Please visit https://api.together.ai/models/togethercomputer/llama-2-7b-chat to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}\n"
     ]
    }
   ],
   "source": [
    "# pass prompt to the llama function, store output as 'response' then print\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a573ca3f-9ffc-4f08-91eb-7e88d99d2f9f",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "[INST]Help me write a birthday card for my dear friend Andrew.[/INST]\n",
      "\n",
      "model: togethercomputer/llama-2-7b-chat\n"
     ]
    }
   ],
   "source": [
    "# Set verbose to True to see the full prompt that is passed to the model.\n",
    "prompt = \"Help me write a birthday card for my dear friend Andrew.\"\n",
    "response = llama(prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0c9bc9-9777-45bb-9e52-1cd99a42e716",
   "metadata": {},
   "source": [
    "### Chat vs. base models\n",
    "\n",
    "Ask model a simple question to demonstrate the different behavior of chat vs. base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b37cc261-a9d5-486c-a0f1-7f0ce93f403e",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "[INST]What is the capital of France?[/INST]\n",
      "\n",
      "model: togethercomputer/llama-2-7b-chat\n"
     ]
    }
   ],
   "source": [
    "### chat model\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = llama(prompt, \n",
    "                 verbose=True,\n",
    "                 model=\"togethercomputer/llama-2-7b-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7d596d0-9469-4220-a03a-b50b0226776d",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Unable to access non-serverless model togethercomputer/llama-2-7b-chat. Please visit https://api.together.ai/models/togethercomputer/llama-2-7b-chat to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71bef6df-ce83-4be7-98a7-cd0c353c4188",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "What is the capital of France?\n",
      "\n",
      "model: togethercomputer/llama-2-7b\n"
     ]
    }
   ],
   "source": [
    "### base model\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = llama(prompt, \n",
    "                 verbose=True,\n",
    "                 add_inst=False,\n",
    "                 model=\"togethercomputer/llama-2-7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf01a1-2c3e-4073-a4b7-546f5de0e5a1",
   "metadata": {},
   "source": [
    "Note how the prompt **does not** include the `[INST]` and `[/INST]` tags as `add_inst` was set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1941be3a-de66-4d08-808e-f93a0393f864",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Unable to access non-serverless model togethercomputer/llama-2-7b. Please visit https://api.together.ai/models/togethercomputer/llama-2-7b to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28242f9a-c36e-423e-af36-d2392c67ac4c",
   "metadata": {},
   "source": [
    "### Using Llama 3 chat models\n",
    "\n",
    "Together.ai supports both Llama 3 8b chat and Llama 3 70b chat models with the following names (case-insensitive):\n",
    "* meta-llama/Llama-3-8b-chat-hf\t\n",
    "* meta-llama/Llama-3-70b-chat-hf\n",
    "\n",
    "You can simply set the `model` parameter to one of the Llama 3 model names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e3d0776-6f1d-49dc-a8eb-302a78e71b01",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "What is the capital of France?\n",
      "\n",
      "model: META-LLAMA/LLAMA-3-8B-CHAT-HF\n"
     ]
    }
   ],
   "source": [
    "response = llama(prompt, \n",
    "                 verbose=True,\n",
    "                 model=\"META-LLAMA/LLAMA-3-8B-CHAT-HF\", \n",
    "                 add_inst=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65423187-e2dc-492a-86fa-e3431238a6d1",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "A) Berlin\n",
      "B) Paris\n",
      "C) London\n",
      "D) Rome\n",
      "\n",
      "Answer: B) Paris\n",
      "\n",
      "**What is the largest planet in our solar system?**\n",
      "A) Earth\n",
      "B) Saturn\n",
      "C) Jupiter\n",
      "D) Uranus\n",
      "\n",
      "Answer: C) Jupiter\n",
      "\n",
      "**What is the smallest country in the world?**\n",
      "A) Vatican City\n",
      "B) Monaco\n",
      "C) Nauru\n",
      "D) Tuvalu\n",
      "\n",
      "Answer: A) Vatican City\n",
      "\n",
      "**What is the largest mammal on Earth?**\n",
      "A) Elephant\n",
      "B) Whale\n",
      "C) Hippopotamus\n",
      "D) Rhinoceros\n",
      "\n",
      "Answer: B) Whale\n",
      "\n",
      "**What is the highest mountain in the world?**\n",
      "A) Mount Everest\n",
      "B) Mount Kilimanjaro\n",
      "C) Mount Denali\n",
      "D) Mount Elbrus\n",
      "\n",
      "Answer: A) Mount Everest\n",
      "\n",
      "**What is the largest river in South America?**\n",
      "A) Amazon River\n",
      "B) Paraná River\n",
      "C) São Francisco River\n",
      "D) Magdalena River\n",
      "\n",
      "Answer: A) Amazon River\n",
      "\n",
      "**What is the largest desert in the world?**\n",
      "A) Sahara Desert\n",
      "B) Gobi Desert\n",
      "C) Mojave Desert\n",
      "D) Atacama Desert\n",
      "\n",
      "Answer: A) Sahara Desert\n",
      "\n",
      "**What is the largest city in the world?**\n",
      "A) Tokyo\n",
      "B) New York City\n",
      "C) London\n",
      "D) Beijing\n",
      "\n",
      "Answer: A) Tokyo\n",
      "\n",
      "**What is the largest lake in the world?**\n",
      "A) Lake Baikal\n",
      "B) Lake Tanganyika\n",
      "C) Lake Superior\n",
      "D) Lake Victoria\n",
      "\n",
      "Answer: A) Lake Baikal\n",
      "\n",
      "**What is the largest waterfall in the world?**\n",
      "A) Victoria Falls\n",
      "B) Iguazu Falls\n",
      "C) Niagara Falls\n",
      "D) Angel Falls\n",
      "\n",
      "Answer: A) Victoria Falls\n",
      "\n",
      "**What is the largest island in the Mediterranean Sea?**\n",
      "A) Sicily\n",
      "B) Sardinia\n",
      "C) Corsica\n",
      "D) Crete\n",
      "\n",
      "Answer: A) Sicily\n",
      "\n",
      "**What is the largest city in Africa?**\n",
      "A) Lagos\n",
      "B) Cairo\n",
      "C) Kinshasa\n",
      "D) Johannesburg\n",
      "\n",
      "Answer: A) Lagos\n",
      "\n",
      "**What is the largest city in Asia?**\n",
      "A) Tokyo\n",
      "B) Seoul\n",
      "C) Shanghai\n",
      "D) Beijing\n",
      "\n",
      "Answer: A) Tokyo\n",
      "\n",
      "**What is the largest city in Europe?**\n",
      "A) Moscow\n",
      "B) London\n",
      "C) Berlin\n",
      "D) Paris\n",
      "\n",
      "Answer: A) Moscow\n",
      "\n",
      "**What is the largest city in North America?**\n",
      "A) New York City\n",
      "B) Los Angeles\n",
      "C) Chicago\n",
      "D) Mexico City\n",
      "\n",
      "Answer: A) New York City\n",
      "\n",
      "**What is the largest city in South America?**\n",
      "A) São Paulo\n",
      "B) Buenos Aires\n",
      "C) Lima\n",
      "D) Rio de Janeiro\n",
      "\n",
      "Answer: A) São Paulo\n",
      "\n",
      "**What is the largest city in Oceania?**\n",
      "A) Sydney\n",
      "B) Melbourne\n",
      "C) Brisbane\n",
      "D) Auckland\n",
      "\n",
      "Answer: A) Sydney\n",
      "\n",
      "**What is the largest city in Antarctica?**\n",
      "A) McMurdo Station\n",
      "B) Palmer Station\n",
      "C) Amundsen-Scott South Pole Station\n",
      "D) Concordia Station\n",
      "\n",
      "Answer: A) McMurdo Station\n",
      "\n",
      "**What is the largest city in the Arctic?**\n",
      "A) Longyearbyen\n",
      "B) Svalbard\n",
      "C) Tromsø\n",
      "D) Murmansk\n",
      "\n",
      "Answer: A) Longyearbyen\n",
      "\n",
      "**What is the largest city in the Antarctic Circle?**\n",
      "A) McMurdo Station\n",
      "B) Palmer Station\n",
      "C) Amundsen-Scott South Pole Station\n",
      "D) Concordia Station\n",
      "\n",
      "Answer: A) McMurdo Station\n",
      "\n",
      "**What is the largest city in the Arctic Circle?**\n",
      "A) Longyearbyen\n",
      "B) Svalbard\n",
      "C) Tromsø\n",
      "D) Murmansk\n",
      "\n",
      "Answer: A) Longyearbyen\n",
      "\n",
      "**What is the largest city in the Antarctic Circle?**\n",
      "A) McMurdo Station\n",
      "B) Palmer Station\n",
      "C) Amundsen-Scott South Pole Station\n",
      "D) Concordia Station\n",
      "\n",
      "Answer: A) McMurdo Station\n",
      "\n",
      "**What is the largest city in the Arctic Circle?**\n",
      "A) Longyearbyen\n",
      "B) Svalbard\n",
      "C) Tromsø\n",
      "D) Murmansk\n",
      "\n",
      "Answer: A) Longyearbyen\n",
      "\n",
      "**What is the largest city in the Antarctic Circle?**\n",
      "A) McMurdo Station\n",
      "B) Palmer Station\n",
      "C) Amundsen-Scott South Pole Station\n",
      "D) Concordia Station\n",
      "\n",
      "Answer: A) McMurdo Station\n",
      "\n",
      "**What is the largest city in the Arctic Circle?**\n",
      "A) Longyearbyen\n",
      "B) Svalbard\n",
      "C) Tromsø\n",
      "D) Murmansk\n",
      "\n",
      "Answer: A) Longyearbyen\n",
      "\n",
      "**What is the largest city in the Antarctic Circle?**\n",
      "A) Mc\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9531475d-430f-4020-9784-1760eb79cbff",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "What is the capital of France?\n",
      "\n",
      "model: META-LLAMA/LLAMA-3-70B-CHAT-HF\n",
      " Paris\n",
      "What is the capital of Germany? Berlin\n",
      "What is the capital of Italy? Rome\n",
      "What is the capital of Spain? Madrid\n",
      "What is the capital of Portugal? Lisbon\n",
      "What is the capital of Switzerland? Bern\n",
      "What is the capital of Austria? Vienna\n",
      "What is the capital of Belgium? Brussels\n",
      "What is the capital of Netherlands? Amsterdam\n",
      "What is the capital of Denmark? Copenhagen\n",
      "What is the capital of Norway? Oslo\n",
      "What is the capital of Sweden? Stockholm\n",
      "What is the capital of Finland? Helsinki\n",
      "What is the capital of Greece? Athens\n",
      "What is the capital of Turkey? Ankara\n",
      "What is the capital of Poland? Warsaw\n",
      "What is the capital of Czech Republic? Prague\n",
      "What is the capital of Hungary? Budapest\n",
      "What is the capital of Romania? Bucharest\n",
      "What is the capital of Bulgaria? Sofia\n",
      "What is the capital of Russia? Moscow\n",
      "What is the capital of Ukraine? Kiev\n",
      "What is the capital of Belarus? Minsk\n",
      "What is the capital of Estonia? Tallinn\n",
      "What is the capital of Latvia? Riga\n",
      "What is the capital of Lithuania? Vilnius\n",
      "What is the capital of Croatia? Zagreb\n",
      "What is the capital of Bosnia and Herzegovina? Sarajevo\n",
      "What is the capital of Serbia? Belgrade\n",
      "What is the capital of Montenegro? Podgorica\n",
      "What is the capital of Albania? Tirana\n",
      "What is the capital of North Macedonia? Skopje\n",
      "What is the capital of Kosovo? Pristina\n",
      "What is the capital of Cyprus? Nicosia\n",
      "What is the capital of Malta? Valletta\n",
      "What is the capital of Iceland? Reykjavik\n",
      "What is the capital of Ireland? Dublin\n",
      "What is the capital of United Kingdom? London\n",
      "What is the capital of Scotland? Edinburgh\n",
      "What is the capital of Wales? Cardiff\n",
      "What is the capital of Northern Ireland? Belfast\n",
      "\n",
      "Note: Some countries have multiple capital cities, a constitutional capital and an administrative capital, but I have only listed the most commonly recognized capital city for each country.\n"
     ]
    }
   ],
   "source": [
    "response = llama(prompt, \n",
    "                 verbose=True,\n",
    "                 model=\"META-LLAMA/LLAMA-3-70B-CHAT-HF\", \n",
    "                 add_inst=False,)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ac25a-95cd-43da-91e4-2d3c885af811",
   "metadata": {},
   "source": [
    "### Changing the temperature setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47c11476-c128-4370-9405-e2899c0284ba",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Unable to access non-serverless model togethercomputer/llama-2-7b-chat. Please visit https://api.together.ai/models/togethercomputer/llama-2-7b-chat to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Help me write a birthday card for my dear friend Andrew.\n",
    "Here are details about my friend:\n",
    "He likes long walks on the beach and reading in the bookstore.\n",
    "His hobbies include reading research papers and speaking at conferences.\n",
    "His favorite color is light blue.\n",
    "He likes pandas.\n",
    "\"\"\"\n",
    "response = llama(prompt, temperature=0.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "065d446f-2eef-4f34-858b-c5146e154cf0",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Unable to access non-serverless model togethercomputer/llama-2-7b-chat. Please visit https://api.together.ai/models/togethercomputer/llama-2-7b-chat to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}\n"
     ]
    }
   ],
   "source": [
    "# Run the code again - the output should be identical\n",
    "response = llama(prompt, temperature=0.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb83e70c-c629-470a-8013-cc63c56b5870",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Unable to access non-serverless model togethercomputer/llama-2-7b-chat. Please visit https://api.together.ai/models/togethercomputer/llama-2-7b-chat to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Help me write a birthday card for my dear friend Andrew.\n",
    "Here are details about my friend:\n",
    "He likes long walks on the beach and reading in the bookstore.\n",
    "His hobbies include reading research papers and speaking at conferences.\n",
    "His favorite color is light blue.\n",
    "He likes pandas.\n",
    "\"\"\"\n",
    "response = llama(prompt, temperature=0.9)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6834c2db-16c7-46ed-8c79-9aac6041fe66",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Unable to access non-serverless model togethercomputer/llama-2-7b-chat. Please visit https://api.together.ai/models/togethercomputer/llama-2-7b-chat to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}\n"
     ]
    }
   ],
   "source": [
    "# run the code again - the output should be different\n",
    "response = llama(prompt, temperature=0.9)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295682fa-ca26-4924-89ca-a86710e64f78",
   "metadata": {},
   "source": [
    "### Changing the max tokens setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ede4dfa8-0d2c-42da-b588-701119bd136f",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Unable to access non-serverless model togethercomputer/llama-2-7b-chat. Please visit https://api.together.ai/models/togethercomputer/llama-2-7b-chat to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Help me write a birthday card for my dear friend Andrew.\n",
    "Here are details about my friend:\n",
    "He likes long walks on the beach and reading in the bookstore.\n",
    "His hobbies include reading research papers and speaking at conferences.\n",
    "His favorite color is light blue.\n",
    "He likes pandas.\n",
    "\"\"\"\n",
    "response = llama(prompt,max_tokens=20)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5333a590-b54a-44f3-a627-1db0fe462315",
   "metadata": {},
   "source": [
    "The next cell reads in the text of the children's book *The Velveteen Rabbit* by Margery Williams, and stores it as a string named `text`. (Note: you can use the File -> Open menu above the notebook to look at this text if you wish.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b0a6d79-8c32-4b56-869d-ec7b4b9e526c",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "with open(\"TheVelveteenRabbit.txt\", \"r\", encoding='utf=8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25e31bf5-b323-4771-ae59-70b73e00ec4b",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Give me a summary of the following text in 50 words:\\n\\n\n",
    "{text}\n",
    "\"\"\"\n",
    "response = llama(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f15ff3c-0068-4abe-8e6f-e3baf92dd31d",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Unable to access non-serverless model togethercomputer/llama-2-7b-chat. Please visit https://api.together.ai/models/togethercomputer/llama-2-7b-chat to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d0f2dc-6822-4636-92cd-d7cd77d9a27f",
   "metadata": {},
   "source": [
    "Running the cell above returns an error because we have too many tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04efbc3e-e67f-4300-a46d-7bf3834077dc",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4998"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum of input tokens (prompt + Velveteen Rabbit text) and output tokens\n",
    "3974 + 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed7ac3-24df-48da-8868-5cddd6176dfe",
   "metadata": {},
   "source": [
    "For Llama 2 chat models, the sum of the input and max_new_tokens parameter must be <= 4097 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee7e1c8d-b16c-41af-8c83-79ce559f860a",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate tokens available for response after accounting for 3974 input tokens\n",
    "4097 - 3974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e93887be-c3ea-44d2-86fc-a4aa9e101a37",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# set max_tokens to stay within limit on input + output tokens\n",
    "prompt = f\"\"\"\n",
    "Give me a summary of the following text in 50 words:\\n\\n\n",
    "{text}\n",
    "\"\"\"\n",
    "response = llama(prompt,\n",
    "                max_tokens=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9e8b6db-e1ac-4480-bd86-d57334d6db57",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Unable to access non-serverless model togethercomputer/llama-2-7b-chat. Please visit https://api.together.ai/models/togethercomputer/llama-2-7b-chat to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ccbe1ce-35f6-4bdb-888a-5232b3a23794",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# increase max_tokens beyond limit on input + output tokens\n",
    "prompt = f\"\"\"\n",
    "Give me a summary of the following text in 50 words:\\n\\n\n",
    "{text}\n",
    "\"\"\"\n",
    "response = llama(prompt,\n",
    "                max_tokens=124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae1e6e7b-0932-4238-9dc1-93dc0891895b",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Unable to access non-serverless model togethercomputer/llama-2-7b-chat. Please visit https://api.together.ai/models/togethercomputer/llama-2-7b-chat to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e768b69e-ee6c-445f-b68b-a7480a76a019",
   "metadata": {},
   "source": [
    "### Asking a follow up question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b613eb4a-c922-4037-ad1e-ffb8cdea1c1f",
   "metadata": {
    "height": 183
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Unable to access non-serverless model togethercomputer/llama-2-7b-chat. Please visit https://api.together.ai/models/togethercomputer/llama-2-7b-chat to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Help me write a birthday card for my dear friend Andrew.\n",
    "Here are details about my friend:\n",
    "He likes long walks on the beach and reading in the bookstore.\n",
    "His hobbies include reading research papers and speaking at conferences.\n",
    "His favorite color is light blue.\n",
    "He likes pandas.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4333b1d1-e4fe-436a-a3af-d23eed09b866",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Unable to access non-serverless model togethercomputer/llama-2-7b-chat. Please visit https://api.together.ai/models/togethercomputer/llama-2-7b-chat to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = \"\"\"\n",
    "Oh, he also likes teaching. Can you rewrite it to include that?\n",
    "\"\"\"\n",
    "response_2 = llama(prompt_2)\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8355e-2da8-411d-8bf1-c0b545ddc4da",
   "metadata": {},
   "source": [
    "### (Optional): Using Llama 2 or 3 on your own computer!\n",
    "- The smaller Llama 2 or 3 chat model is free to download on your own machine!\n",
    "  - **Note** that only the Llama 2 7B chat or Llama 3 8B model (by default the 4-bit quantized version is downloaded) may work fine locally.\n",
    "  - Other larger sized models could require too much memory (13b models generally require at least 16GB of RAM and 70b models at least 64GB of RAM) and run too slowly.\n",
    "  - The Meta team still recommends using a hosted API service (in this case, the classroom is using Together.AI as hosted API service) because it allows you to access all the available llama models without being limited by your hardware.\n",
    "  - You can find more instructions on using the Together.AI API service outside of the classroom if you go to the last lesson of this short course. \n",
    "- One way to install and use llama 7B on your computer is to go to https://ollama.com/ and download app. It will be like installing a regular application.\n",
    "- To use Llama 2 or 3, the full instructions are here: https://ollama.com/library/llama2 and https://ollama.com/library/llama3.\n",
    "\n",
    "\n",
    "#### Here's an quick summary of how to get started:\n",
    "  - Follow the installation instructions (for Windows, Mac or Linux).\n",
    "  - Open the command line interface (CLI) and type `ollama run llama2` or `ollama run llama3`. \n",
    "  - The first time you do this, it will take some time to download the llama 2 or 3 model. After that, you'll see \n",
    "> `>>> Send a message (/? for help)`\n",
    "\n",
    "- You can type your prompt and the llama-2 model on your computer will give you a response!\n",
    "- To exit, type `/bye`.\n",
    "- For a list of other commands, type `/?`.\n",
    "\n",
    "![](ollama_example.png \"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71958a71-89d9-4ccb-88e2-97fa173bb09e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
